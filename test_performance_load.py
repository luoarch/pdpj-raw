#!/usr/bin/env python3
"""
Script para testes de performance e carga do sistema PDPJ.

Este script testa:
1. Performance de endpoints individuais
2. Carga com m√∫ltiplas requisi√ß√µes simult√¢neas
3. Limites de rate limiting
4. Performance de downloads em lote
5. Uso de mem√≥ria e CPU
6. Tempo de resposta sob carga
7. Estabilidade do sistema
"""

import asyncio
import time
import psutil
import statistics
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import aiohttp
import httpx
from loguru import logger

# Configurar logging
logger.remove()
logger.add(
    "logs/performance_test.log",
    rotation="10 MB",
    retention="7 days",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}"
)
logger.add(
    lambda msg: print(msg, end=""),
    level="INFO",
    format="<green>{time:HH:mm:ss}</green> | <level>{level}</level> | {message}"
)

@dataclass
class PerformanceResult:
    """Resultado de um teste de performance."""
    test_name: str
    success: bool
    duration: float
    requests_count: int
    success_rate: float
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    p95_response_time: float
    p99_response_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    errors: List[str]
    details: Dict[str, Any]

class PerformanceTester:
    """Classe para executar testes de performance e carga."""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url.rstrip('/')
        self.results: List[PerformanceResult] = []
        self.session = None
        
    async def __aenter__(self):
        """Context manager entry."""
        self.session = httpx.AsyncClient(
            timeout=httpx.Timeout(60.0),
            limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        if self.session:
            await self.session.aclose()
    
    def get_system_metrics(self) -> Tuple[float, float]:
        """Obter m√©tricas do sistema (CPU e mem√≥ria)."""
        try:
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory = psutil.virtual_memory()
            memory_mb = memory.used / 1024 / 1024
            return cpu_percent, memory_mb
        except Exception as e:
            logger.warning(f"Erro ao obter m√©tricas do sistema: {e}")
            return 0.0, 0.0
    
    def calculate_percentiles(self, values: List[float]) -> Dict[str, float]:
        """Calcular percentis de uma lista de valores."""
        if not values:
            return {"p50": 0, "p95": 0, "p99": 0}
        
        sorted_values = sorted(values)
        n = len(sorted_values)
        
        return {
            "p50": sorted_values[int(n * 0.5)],
            "p95": sorted_values[int(n * 0.95)],
            "p99": sorted_values[int(n * 0.99)]
        }
    
    async def test_single_endpoint_performance(self, endpoint: str, iterations: int = 10) -> PerformanceResult:
        """Testar performance de um endpoint individual."""
        logger.info(f"üîç Testando performance do endpoint: {endpoint}")
        
        start_time = time.time()
        response_times = []
        errors = []
        success_count = 0
        
        for i in range(iterations):
            try:
                request_start = time.time()
                response = await self.session.get(f"{self.base_url}{endpoint}")
                request_duration = time.time() - request_start
                
                if response.status_code == 200:
                    success_count += 1
                    response_times.append(request_duration)
                else:
                    errors.append(f"HTTP {response.status_code}")
                    
            except Exception as e:
                errors.append(str(e))
        
        total_duration = time.time() - start_time
        cpu_usage, memory_usage = self.get_system_metrics()
        
        # Calcular estat√≠sticas
        avg_response_time = statistics.mean(response_times) if response_times else 0
        min_response_time = min(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        percentiles = self.calculate_percentiles(response_times)
        
        result = PerformanceResult(
            test_name=f"Single Endpoint: {endpoint}",
            success=success_count > 0,
            duration=total_duration,
            requests_count=iterations,
            success_rate=(success_count / iterations) * 100,
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=percentiles["p95"],
            p99_response_time=percentiles["p99"],
            memory_usage_mb=memory_usage,
            cpu_usage_percent=cpu_usage,
            errors=errors,
            details={
                "endpoint": endpoint,
                "iterations": iterations,
                "success_count": success_count,
                "response_times": response_times[:5]  # Primeiros 5 para debug
            }
        )
        
        self.results.append(result)
        return result
    
    async def test_concurrent_requests(self, endpoint: str, concurrent_users: int, requests_per_user: int) -> PerformanceResult:
        """Testar requisi√ß√µes concorrentes."""
        logger.info(f"‚ö° Testando {concurrent_users} usu√°rios concorrentes, {requests_per_user} req/usu√°rio")
        
        async def user_session():
            """Simular uma sess√£o de usu√°rio."""
            user_response_times = []
            user_errors = []
            
            for _ in range(requests_per_user):
                try:
                    request_start = time.time()
                    response = await self.session.get(f"{self.base_url}{endpoint}")
                    request_duration = time.time() - request_start
                    
                    if response.status_code == 200:
                        user_response_times.append(request_duration)
                    else:
                        user_errors.append(f"HTTP {response.status_code}")
                        
                except Exception as e:
                    user_errors.append(str(e))
            
            return user_response_times, user_errors
        
        start_time = time.time()
        
        # Executar usu√°rios concorrentes
        tasks = [user_session() for _ in range(concurrent_users)]
        user_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_duration = time.time() - start_time
        cpu_usage, memory_usage = self.get_system_metrics()
        
        # Consolidar resultados
        all_response_times = []
        all_errors = []
        success_count = 0
        
        for result in user_results:
            if isinstance(result, Exception):
                all_errors.append(str(result))
            else:
                response_times, errors = result
                all_response_times.extend(response_times)
                all_errors.extend(errors)
                success_count += len(response_times)
        
        total_requests = concurrent_users * requests_per_user
        
        # Calcular estat√≠sticas
        avg_response_time = statistics.mean(all_response_times) if all_response_times else 0
        min_response_time = min(all_response_times) if all_response_times else 0
        max_response_time = max(all_response_times) if all_response_times else 0
        percentiles = self.calculate_percentiles(all_response_times)
        
        result = PerformanceResult(
            test_name=f"Concurrent Load: {concurrent_users} users",
            success=success_count > 0,
            duration=total_duration,
            requests_count=total_requests,
            success_rate=(success_count / total_requests) * 100,
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=percentiles["p95"],
            p99_response_time=percentiles["p99"],
            memory_usage_mb=memory_usage,
            cpu_usage_percent=cpu_usage,
            errors=all_errors[:10],  # Primeiros 10 erros
            details={
                "concurrent_users": concurrent_users,
                "requests_per_user": requests_per_user,
                "total_requests": total_requests,
                "success_count": success_count,
                "throughput_rps": total_requests / total_duration if total_duration > 0 else 0
            }
        )
        
        self.results.append(result)
        return result
    
    async def test_rate_limiting(self, endpoint: str, requests_per_second: int, duration_seconds: int) -> PerformanceResult:
        """Testar rate limiting."""
        logger.info(f"üö¶ Testando rate limiting: {requests_per_second} req/s por {duration_seconds}s")
        
        start_time = time.time()
        response_times = []
        errors = []
        success_count = 0
        rate_limited_count = 0
        
        request_interval = 1.0 / requests_per_second
        
        while time.time() - start_time < duration_seconds:
            try:
                request_start = time.time()
                response = await self.session.get(f"{self.base_url}{endpoint}")
                request_duration = time.time() - request_start
                
                if response.status_code == 200:
                    success_count += 1
                    response_times.append(request_duration)
                elif response.status_code == 429:
                    rate_limited_count += 1
                    errors.append("Rate Limited (429)")
                else:
                    errors.append(f"HTTP {response.status_code}")
                    
            except Exception as e:
                errors.append(str(e))
            
            # Aguardar intervalo para manter taxa
            await asyncio.sleep(request_interval)
        
        total_duration = time.time() - start_time
        cpu_usage, memory_usage = self.get_system_metrics()
        
        total_requests = success_count + rate_limited_count + len(errors)
        
        # Calcular estat√≠sticas
        avg_response_time = statistics.mean(response_times) if response_times else 0
        min_response_time = min(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        percentiles = self.calculate_percentiles(response_times)
        
        result = PerformanceResult(
            test_name=f"Rate Limiting: {requests_per_second} req/s",
            success=success_count > 0,
            duration=total_duration,
            requests_count=total_requests,
            success_rate=(success_count / total_requests) * 100 if total_requests > 0 else 0,
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=percentiles["p95"],
            p99_response_time=percentiles["p99"],
            memory_usage_mb=memory_usage,
            cpu_usage_percent=cpu_usage,
            errors=errors[:10],
            details={
                "target_rps": requests_per_second,
                "duration_seconds": duration_seconds,
                "success_count": success_count,
                "rate_limited_count": rate_limited_count,
                "actual_rps": total_requests / total_duration if total_duration > 0 else 0
            }
        )
        
        self.results.append(result)
        return result
    
    async def test_memory_stress(self, endpoint: str, duration_seconds: int = 30) -> PerformanceResult:
        """Testar uso de mem√≥ria sob stress."""
        logger.info(f"üß† Teste de stress de mem√≥ria por {duration_seconds}s")
        
        start_time = time.time()
        response_times = []
        errors = []
        success_count = 0
        memory_samples = []
        
        while time.time() - start_time < duration_seconds:
            try:
                request_start = time.time()
                response = await self.session.get(f"{self.base_url}{endpoint}")
                request_duration = time.time() - request_start
                
                if response.status_code == 200:
                    success_count += 1
                    response_times.append(request_duration)
                else:
                    errors.append(f"HTTP {response.status_code}")
                    
            except Exception as e:
                errors.append(str(e))
            
            # Amostrar mem√≥ria a cada 5 segundos
            if len(memory_samples) == 0 or time.time() - start_time > len(memory_samples) * 5:
                _, memory_usage = self.get_system_metrics()
                memory_samples.append(memory_usage)
            
            # Pequena pausa para n√£o sobrecarregar
            await asyncio.sleep(0.1)
        
        total_duration = time.time() - start_time
        cpu_usage, final_memory = self.get_system_metrics()
        
        # Calcular estat√≠sticas de mem√≥ria
        max_memory = max(memory_samples) if memory_samples else final_memory
        avg_memory = statistics.mean(memory_samples) if memory_samples else final_memory
        
        # Calcular estat√≠sticas de resposta
        avg_response_time = statistics.mean(response_times) if response_times else 0
        min_response_time = min(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        percentiles = self.calculate_percentiles(response_times)
        
        result = PerformanceResult(
            test_name=f"Memory Stress: {duration_seconds}s",
            success=success_count > 0,
            duration=total_duration,
            requests_count=success_count + len(errors),
            success_rate=(success_count / (success_count + len(errors))) * 100 if (success_count + len(errors)) > 0 else 0,
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=percentiles["p95"],
            p99_response_time=percentiles["p99"],
            memory_usage_mb=max_memory,
            cpu_usage_percent=cpu_usage,
            errors=errors[:10],
            details={
                "duration_seconds": duration_seconds,
                "success_count": success_count,
                "memory_samples": len(memory_samples),
                "max_memory_mb": max_memory,
                "avg_memory_mb": avg_memory,
                "memory_growth_mb": max_memory - (memory_samples[0] if memory_samples else 0)
            }
        )
        
        self.results.append(result)
        return result
    
    def print_result(self, result: PerformanceResult):
        """Imprimir resultado de um teste."""
        status = "‚úÖ PASSOU" if result.success else "‚ùå FALHOU"
        
        logger.info(f"{status} | {result.test_name}")
        logger.info(f"   ‚è±Ô∏è  Dura√ß√£o: {result.duration:.2f}s")
        logger.info(f"   üìä Requisi√ß√µes: {result.requests_count}")
        logger.info(f"   ‚úÖ Taxa de sucesso: {result.success_rate:.1f}%")
        logger.info(f"   ‚ö° Tempo m√©dio: {result.avg_response_time*1000:.1f}ms")
        logger.info(f"   üìà P95: {result.p95_response_time*1000:.1f}ms, P99: {result.p99_response_time*1000:.1f}ms")
        logger.info(f"   üß† Mem√≥ria: {result.memory_usage_mb:.1f}MB")
        logger.info(f"   üíª CPU: {result.cpu_usage_percent:.1f}%")
        
        if result.errors:
            logger.warning(f"   ‚ö†Ô∏è  Erros: {len(result.errors)} (primeiros: {result.errors[:3]})")
        
        # Detalhes espec√≠ficos
        if "throughput_rps" in result.details:
            logger.info(f"   üöÄ Throughput: {result.details['throughput_rps']:.1f} req/s")
        
        if "rate_limited_count" in result.details:
            logger.info(f"   üö¶ Rate Limited: {result.details['rate_limited_count']} vezes")
        
        if "memory_growth_mb" in result.details:
            logger.info(f"   üìà Crescimento mem√≥ria: {result.details['memory_growth_mb']:.1f}MB")
    
    def print_summary(self):
        """Imprimir resumo de todos os testes."""
        if not self.results:
            logger.warning("Nenhum resultado para exibir")
            return
        
        logger.info("\n" + "="*80)
        logger.info("üìä RESUMO DOS TESTES DE PERFORMANCE E CARGA")
        logger.info("="*80)
        
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results if r.success)
        total_duration = sum(r.duration for r in self.results)
        
        logger.info(f"‚è±Ô∏è  Dura√ß√£o total: {total_duration:.2f} segundos")
        logger.info(f"üìà Total de testes: {total_tests}")
        logger.info(f"‚úÖ Testes aprovados: {passed_tests}")
        logger.info(f"‚ùå Testes falharam: {total_tests - passed_tests}")
        logger.info(f"üìä Taxa de sucesso: {(passed_tests/total_tests)*100:.1f}%")
        
        # Estat√≠sticas de performance
        all_avg_times = [r.avg_response_time for r in self.results if r.avg_response_time > 0]
        all_success_rates = [r.success_rate for r in self.results]
        
        if all_avg_times:
            logger.info(f"\n‚ö° PERFORMANCE:")
            logger.info(f"   ‚Ä¢ Tempo m√©dio de resposta: {statistics.mean(all_avg_times)*1000:.1f}ms")
            logger.info(f"   ‚Ä¢ Melhor tempo: {min(all_avg_times)*1000:.1f}ms")
            logger.info(f"   ‚Ä¢ Pior tempo: {max(all_avg_times)*1000:.1f}ms")
        
        if all_success_rates:
            logger.info(f"\nüìä CONFIABILIDADE:")
            logger.info(f"   ‚Ä¢ Taxa de sucesso m√©dia: {statistics.mean(all_success_rates):.1f}%")
            logger.info(f"   ‚Ä¢ Melhor taxa: {max(all_success_rates):.1f}%")
            logger.info(f"   ‚Ä¢ Pior taxa: {min(all_success_rates):.1f}%")
        
        # Uso de recursos
        max_memory = max(r.memory_usage_mb for r in self.results)
        max_cpu = max(r.cpu_usage_percent for r in self.results)
        
        logger.info(f"\nüíª RECURSOS:")
        logger.info(f"   ‚Ä¢ Pico de mem√≥ria: {max_memory:.1f}MB")
        logger.info(f"   ‚Ä¢ Pico de CPU: {max_cpu:.1f}%")
        
        # Recomenda√ß√µes
        logger.info(f"\nüí° RECOMENDA√á√ïES:")
        
        if passed_tests == total_tests:
            logger.success("üéâ EXCELENTE! Todos os testes passaram!")
        elif passed_tests >= total_tests * 0.8:
            logger.info("‚úÖ BOM! Maioria dos testes passou, alguns ajustes podem ser necess√°rios.")
        else:
            logger.warning("‚ö†Ô∏è ATEN√á√ÉO! Muitos testes falharam, revis√£o necess√°ria.")
        
        # Verificar performance
        if all_avg_times and statistics.mean(all_avg_times) > 1.0:
            logger.warning("‚ö†Ô∏è Tempos de resposta altos detectados (>1s)")
        
        if max_memory > 1000:
            logger.warning("‚ö†Ô∏è Alto uso de mem√≥ria detectado (>1GB)")
        
        if max_cpu > 80:
            logger.warning("‚ö†Ô∏è Alto uso de CPU detectado (>80%)")

async def run_all_tests():
    """Executar todos os testes de performance."""
    logger.info("üöÄ INICIANDO TESTES DE PERFORMANCE E CARGA")
    logger.info("="*60)
    
    async with PerformanceTester() as tester:
        # Teste 1: Performance de endpoints individuais
        logger.info("\nüîç FASE 1: TESTES DE ENDPOINTS INDIVIDUAIS")
        logger.info("-" * 50)
        
        endpoints = [
            "/health",
            "/api/v1/processes",
            "/api/v1/monitoring/status"
        ]
        
        for endpoint in endpoints:
            try:
                result = await tester.test_single_endpoint_performance(endpoint, iterations=20)
                tester.print_result(result)
            except Exception as e:
                logger.error(f"Erro no teste {endpoint}: {e}")
        
        # Teste 2: Carga concorrente
        logger.info("\n‚ö° FASE 2: TESTES DE CARGA CONCORRENTE")
        logger.info("-" * 50)
        
        load_scenarios = [
            (5, 10),   # 5 usu√°rios, 10 req cada
            (10, 5),   # 10 usu√°rios, 5 req cada
            (20, 3),   # 20 usu√°rios, 3 req cada
        ]
        
        for users, req_per_user in load_scenarios:
            try:
                result = await tester.test_concurrent_requests("/health", users, req_per_user)
                tester.print_result(result)
            except Exception as e:
                logger.error(f"Erro no teste de carga {users}x{req_per_user}: {e}")
        
        # Teste 3: Rate limiting
        logger.info("\nüö¶ FASE 3: TESTES DE RATE LIMITING")
        logger.info("-" * 50)
        
        rate_scenarios = [
            (10, 10),  # 10 req/s por 10s
            (20, 10),  # 20 req/s por 10s
            (50, 5),   # 50 req/s por 5s
        ]
        
        for rps, duration in rate_scenarios:
            try:
                result = await tester.test_rate_limiting("/health", rps, duration)
                tester.print_result(result)
            except Exception as e:
                logger.error(f"Erro no teste de rate limiting {rps}req/s: {e}")
        
        # Teste 4: Stress de mem√≥ria
        logger.info("\nüß† FASE 4: TESTES DE STRESS DE MEM√ìRIA")
        logger.info("-" * 50)
        
        try:
            result = await tester.test_memory_stress("/health", duration_seconds=20)
            tester.print_result(result)
        except Exception as e:
            logger.error(f"Erro no teste de stress de mem√≥ria: {e}")
        
        # Resumo final
        tester.print_summary()

def main():
    """Fun√ß√£o principal."""
    try:
        asyncio.run(run_all_tests())
    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è Testes interrompidos pelo usu√°rio")
    except Exception as e:
        logger.error(f"‚ùå Erro durante os testes: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
